# Nagarro's Ascent Platform - Data Flow & Database Structure

## Overview
This document provides a comprehensive overview of the data flow and database structure for Nagarro's Ascent migration platform, with detailed focus on document generation and chat interaction capabilities.

---

## üèóÔ∏è Platform Architecture

### Service Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ    ‚îÇ    Backend      ‚îÇ    ‚îÇ Project Service ‚îÇ
‚îÇ   (React)       ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (FastAPI)     ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (FastAPI)     ‚îÇ
‚îÇ   Port: 3000    ‚îÇ    ‚îÇ   Port: 8000    ‚îÇ    ‚îÇ   Port: 8002    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ Reporting Svc   ‚îÇ
                       ‚îÇ   (FastAPI)     ‚îÇ
                       ‚îÇ   Port: 8001    ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Layer Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL    ‚îÇ    ‚îÇ    Weaviate     ‚îÇ    ‚îÇ     Neo4j       ‚îÇ
‚îÇ  (Relational)   ‚îÇ    ‚îÇ   (Vector DB)   ‚îÇ    ‚îÇ   (Graph DB)    ‚îÇ
‚îÇ   Port: 5432    ‚îÇ    ‚îÇ   Port: 8080    ‚îÇ    ‚îÇ   Port: 7687    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ     MinIO       ‚îÇ
                       ‚îÇ (Object Store)  ‚îÇ
                       ‚îÇ   Port: 9000    ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Document Generation Data Flow

### 1. Frontend Initiation
```typescript
// DocumentTemplates.tsx
const handleGenerateDocument = async (template: DocumentTemplate) => {
  const response = await fetch(`http://localhost:8000/api/projects/${projectId}/generate-document`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      name: template.name,
      description: template.description,
      format: template.format,
      output_type: template.output_type,
    }),
  });
};
```

### 2. Backend Processing Pipeline
```python
# backend/app/main.py
@app.post("/api/projects/{project_id}/generate-document")
async def generate_document(project_id: str, request: dict):
    # Step 1: Validate project and get LLM configuration
    project_service = get_project_service()
    project = project_service.get_project(project_id)

    # Step 2: Initialize LLM from project configuration
    llm = get_project_llm(project)

    # Step 3: Create document generation crew
    crew = create_document_generation_crew(
        project_id=project_id,
        llm=llm,
        document_type=request.get('name'),
        document_description=request.get('description'),
        output_format=request.get('format', 'markdown')
    )

    # Step 4: Execute crew to generate document
    result = await asyncio.to_thread(crew.kickoff)

    # Step 5: Save and process document
    # Save markdown locally
    # Generate PDF/DOCX via reporting service
    # Store in MinIO
    # Return download URLs
```

### 3. CrewAI Agent Pipeline
```python
# backend/app/core/crew.py
def create_document_generation_crew(project_id, llm, document_type, document_description, output_format='markdown', websocket=None):
    # Initialize services
    rag_service = RAGService(project_id, llm)
    rag_tool = RAGQueryTool(rag_service=rag_service)
    graph_service = GraphService()
    graph_tool = GraphQueryTool(graph_service=graph_service)

    # Agent 1: Research Specialist
    research_agent = Agent(
        role='Research Specialist',
        goal='Extract and analyze relevant information from project documents',
        tools=[rag_tool, graph_tool]
    )

    # Agent 2: Content Architect
    content_agent = Agent(
        role='Content Architect',
        goal='Structure and organize content into professional document format',
        tools=[rag_tool, graph_tool]
    )

    # Agent 3: Quality Reviewer
    quality_agent = Agent(
        role='Quality Reviewer',
        goal='Review and enhance document quality and completeness',
        tools=[rag_tool]
    )

    # Create crew with sequential execution
    return Crew(
        agents=[research_agent, content_agent, quality_agent],
        tasks=[research_task, content_task, quality_task],
        verbose=True,
        callbacks=[log_handler] if websocket else None
    )
```

### 4. RAG Service Integration
```python
# backend/app/core/rag_service.py
class RAGService:
    def query(self, question: str, n_results: int = 5):
        # Step 1: Create question embedding
        question_embedding = self.embedding_model.encode([question])[0]

        # Step 2: Vector search in Weaviate
        if self.weaviate_client:
            collection = self.weaviate_client.collections.get(self.class_name)
            response = collection.query.near_vector(
                near_vector=question_embedding,
                limit=n_results,
                return_metadata=['distance']
            )

        # Step 3: Extract relevant documents
        docs = []
        for item in response.objects:
            content = item.properties.get('content', '')
            filename = item.properties.get('filename', 'unknown')
            docs.append(f"[From {filename}]: {content}")

        # Step 4: Synthesize response using LLM
        if self.llm and docs:
            return self._synthesize_response(question, docs)
        else:
            return "\n\n".join(docs)
```

### 5. Professional Report Generation
```python
# reporting-service/main.py
@app.post("/generate_report")
async def generate_report(request: ReportRequest):
    # Step 1: Generate PDF using WeasyPrint
    pdf_content = _generate_pdf(request.markdown_content)

    # Step 2: Generate DOCX using pypandoc
    docx_content = _generate_docx(request.markdown_content)

    # Step 3: Store in MinIO
    pdf_filename = f"{request.project_id}_{request.document_name}_{timestamp}.pdf"
    docx_filename = f"{request.project_id}_{request.document_name}_{timestamp}.docx"

    minio_client.put_object(bucket_name, pdf_filename, pdf_content)
    minio_client.put_object(bucket_name, docx_filename, docx_content)

    # Step 4: Save locally
    local_pdf_path = f"./reports/{pdf_filename}"
    local_docx_path = f"./reports/{docx_filename}"

    # Step 5: Return download URLs
    return {
        "pdf_url": f"/api/reports/download/{pdf_filename}",
        "docx_url": f"/api/reports/download/{docx_filename}",
        "local_pdf_path": local_pdf_path,
        "local_docx_path": local_docx_path
    }
```

---

## üí¨ Chat Interaction Data Flow

### 1. Chat Interface Initiation
```typescript
// ChatInterface.tsx / FloatingChatWidget.tsx
const handleSendMessage = async (content: string) => {
  const userMessage: Message = {
    id: Date.now().toString(),
    type: 'user',
    content,
    timestamp: new Date(),
  };

  setMessages(prev => [...prev, userMessage]);
  setLoading(true);

  try {
    // Query knowledge base via RAG service
    const response = await apiService.queryKnowledgeBase(projectId, content);

    const assistantMessage: Message = {
      id: (Date.now() + 1).toString(),
      type: 'assistant',
      content: response.answer || 'No relevant information found.',
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, assistantMessage]);
  } catch (error) {
    // Handle error
  }
};
```

### 2. Backend Query Processing
```python
# backend/app/main.py
@app.post("/api/projects/{project_id}/query")
async def query_project_knowledge(project_id: str, request: dict):
    # Step 1: Validate project
    project_service = get_project_service()
    project = project_service.get_project(project_id)

    # Step 2: Get project LLM configuration
    llm = get_project_llm(project)

    # Step 3: Initialize RAG service
    rag_service = RAGService(project_id, llm)

    # Step 4: Query knowledge base
    answer = rag_service.query(request.get('question', ''))

    return {
        "answer": answer,
        "project_id": project_id,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }
```

### 3. Multi-Modal Knowledge Retrieval
```python
# RAG Service Query Pipeline
def query(self, question: str, n_results: int = 5):
    # Vector Search (Semantic Similarity)
    vector_results = self._vector_search(question, n_results)

    # Graph Search (Relationship Context)
    graph_context = self._graph_search(question)

    # Combine and synthesize
    combined_context = self._combine_contexts(vector_results, graph_context)

    # Generate response using LLM
    response = self._synthesize_response(question, combined_context)

    return response
```

---

## üìä Database Structure

### PostgreSQL Schema (Project Service)

#### Projects Table
```sql
CREATE TABLE projects (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    client_name VARCHAR(255) NOT NULL,
    client_contact VARCHAR(255),
    status VARCHAR(50) NOT NULL DEFAULT 'initiated',
    report_url VARCHAR(500),
    report_content TEXT,
    report_artifact_url VARCHAR(500),

    -- LLM Configuration
    llm_provider VARCHAR(50),           -- openai, anthropic, gemini, ollama
    llm_model VARCHAR(100),             -- gpt-4o, claude-3-5-sonnet, gemini-2.0-flash
    llm_api_key_id VARCHAR(255),        -- Reference to LLM configuration
    llm_temperature VARCHAR(10) DEFAULT '0.1',
    llm_max_tokens VARCHAR(10) DEFAULT '4000',

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### LLM Configurations Table
```sql
CREATE TABLE llm_configurations (
    id VARCHAR(255) PRIMARY KEY,       -- Custom ID like "gemini1_1754014595"
    name VARCHAR(255) NOT NULL,        -- User-friendly name
    provider VARCHAR(50) NOT NULL,     -- openai, gemini, anthropic, etc.
    model VARCHAR(100) NOT NULL,       -- gpt-4o, gemini-1.5-pro, etc.
    api_key TEXT NOT NULL,             -- Encrypted API key
    temperature VARCHAR(10) NOT NULL DEFAULT '0.1',
    max_tokens VARCHAR(10) NOT NULL DEFAULT '4000',
    description TEXT,
    created_by UUID NOT NULL REFERENCES users(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Project Files Table
```sql
CREATE TABLE project_files (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    filename VARCHAR(255) NOT NULL,
    file_type VARCHAR(100),
    upload_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE
);
```

#### Users Table
```sql
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) UNIQUE NOT NULL,
    hashed_password VARCHAR(255) NOT NULL,
    role VARCHAR(50) NOT NULL DEFAULT 'user',
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### Weaviate Schema (Vector Database)

#### Document Collection Structure
```python
# Collection: Project_{project_id}
{
    "class": "Project_45ea6c9c-b620-4235-86a7-79011c97275f",
    "properties": [
        {
            "name": "content",
            "dataType": ["text"],
            "description": "Document content chunk"
        },
        {
            "name": "filename",
            "dataType": ["string"],
            "description": "Source filename"
        },
        {
            "name": "chunk_id",
            "dataType": ["string"],
            "description": "Unique chunk identifier"
        },
        {
            "name": "project_id",
            "dataType": ["string"],
            "description": "Project identifier"
        }
    ],
    "vectorizer": "text2vec-transformers",
    "moduleConfig": {
        "text2vec-transformers": {
            "model": "sentence-transformers/all-MiniLM-L6-v2",
            "dimensions": 384
        }
    }
}
```

### Neo4j Schema (Graph Database)

#### Node Types
```cypher
// Server Nodes
CREATE (s:Server {
    name: 'web-server',
    type: 'application_server',
    project_id: '45ea6c9c-b620-4235-86a7-79011c97275f',
    ip_address: '192.168.1.100',
    os: 'Linux',
    created_at: datetime()
})

// Application Nodes
CREATE (a:Application {
    name: 'web-application',
    type: 'web_app',
    project_id: '45ea6c9c-b620-4235-86a7-79011c97275f',
    technology: 'Java Spring',
    version: '2.7.0',
    created_at: datetime()
})

// Database Nodes
CREATE (d:Database {
    name: 'user-database',
    type: 'postgresql',
    project_id: '45ea6c9c-b620-4235-86a7-79011c97275f',
    version: '13.0',
    size_gb: 50,
    created_at: datetime()
})
```

#### Relationship Types
```cypher
// Server hosts Application
MATCH (s:Server {name: 'web-server'}), (a:Application {name: 'web-application'})
CREATE (s)-[:HOSTS {since: datetime(), port: 8080}]->(a)

// Application connects to Database
MATCH (a:Application {name: 'web-application'}), (d:Database {name: 'user-database'})
CREATE (a)-[:CONNECTS_TO {connection_type: 'JDBC', pool_size: 10}]->(d)

// Application depends on Service
MATCH (a1:Application {name: 'web-application'}), (a2:Application {name: 'auth-service'})
CREATE (a1)-[:DEPENDS_ON {dependency_type: 'API', criticality: 'HIGH'}]->(a2)
```

### MinIO Object Storage Structure

#### Bucket Organization
```
reports/
‚îú‚îÄ‚îÄ projects/
‚îÇ   ‚îî‚îÄ‚îÄ {project_id}/
‚îÇ       ‚îú‚îÄ‚îÄ documents/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ {document_name}_{timestamp}.pdf
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ {document_name}_{timestamp}.docx
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ {document_name}_{timestamp}.md
‚îÇ       ‚îú‚îÄ‚îÄ assessments/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ infrastructure_report_{timestamp}.pdf
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ migration_plan_{timestamp}.docx
‚îÇ       ‚îî‚îÄ‚îÄ uploads/
‚îÇ           ‚îú‚îÄ‚îÄ original_file_1.txt
‚îÇ           ‚îî‚îÄ‚îÄ original_file_2.pdf
```

---

## üîÑ Document Processing Pipeline

### 1. File Upload & Initial Processing
```python
# When documents are uploaded
@app.post("/api/projects/{project_id}/upload")
async def upload_files(project_id: str, files: List[UploadFile]):
    for file in files:
        # Step 1: Save file locally
        file_path = f"uploads/project_{project_id}/{file.filename}"

        # Step 2: Record in project service database
        project_service.add_project_file(project_id, {
            "filename": file.filename,
            "file_type": file.content_type,
            "upload_timestamp": datetime.now()
        })

        # Step 3: Mark for processing
        # Files are NOT automatically processed - requires explicit "Start Processing"
```

### 2. Document Processing (On-Demand)
```python
# When "Start Processing" is clicked
@app.websocket("/ws/run_assessment/{project_id}")
async def run_assessment_ws(websocket: WebSocket, project_id: str):
    # Step 1: Check if already processed
    processing_stats_file = f"projects/project_{project_id}/processing_stats.json"

    if os.path.exists(processing_stats_file):
        # QUESTION: Should we reprocess or skip?
        # Current behavior: REPROCESSES (creates duplicates)
        # Recommended: ASK USER or CHECK TIMESTAMPS
        await websocket.send_text("‚ö†Ô∏è Documents already processed. Reprocessing will create duplicates.")

    # Step 2: Process each file
    for file in files:
        # Extract text content
        content = extract_text(file_path)

        # Create embeddings and store in Weaviate
        chunks = rag_service._split_content(content)
        embeddings = rag_service._create_embeddings(chunks)
        rag_service._store_embeddings(embeddings)

        # Extract entities and store in Neo4j
        entities = rag_service.extract_and_add_entities(content)

        # Update processing stats
        stats = {
            "embeddings": len(embeddings),
            "entities": len(entities),
            "processed_at": datetime.now().isoformat(),
            "file_count": len(files)
        }
```

### 3. Duplicate Processing Issue - FIXED ‚úÖ
**Previous Problem**: When clicking "Start Processing" multiple times, the system created duplicate embeddings and entities.

**Current Solution**:
- ‚úÖ **Deduplication Check**: System checks for existing `processing_stats.json` before processing
- ‚úÖ **Data Cleanup**: Clears existing Weaviate embeddings and Neo4j entities before reprocessing
- ‚úÖ **Timestamp Validation**: Compares file upload times with last processing time
- ‚úÖ **Processing Statistics**: Saves processing metadata to prevent unnecessary reprocessing

**Implementation**:
```python
def should_reprocess_project(project_id: str) -> bool:
    """Check if project needs reprocessing"""
    stats_file = f"projects/project_{project_id}/processing_stats.json"

    if not os.path.exists(stats_file):
        return True  # Never processed

    # Check if new files added since last processing
    with open(stats_file, 'r') as f:
        stats = json.load(f)

    last_processed = datetime.fromisoformat(stats['processed_at'])

    # Check for newer files
    project_files = get_project_files(project_id)
    for file in project_files:
        if file.upload_timestamp > last_processed:
            return True  # New files added

    return False  # No reprocessing needed
```

---

## ü§ñ Agent Interaction Capabilities

### 1. Agent Architecture
```python
# Research Specialist Agent
research_agent = Agent(
    role='Research Specialist',
    goal='Extract and analyze relevant information from project documents using RAG and knowledge graph',
    backstory='Expert in document analysis and information extraction with deep understanding of IT infrastructure',
    tools=[rag_tool, graph_tool],
    llm=llm,
    verbose=True,
    callbacks=[log_handler]
)

# Content Architect Agent
content_agent = Agent(
    role='Content Architect',
    goal='Structure and organize extracted information into professional document format',
    backstory='Experienced technical writer specializing in migration documentation and enterprise architecture',
    tools=[rag_tool, graph_tool],
    llm=llm,
    verbose=True,
    callbacks=[log_handler]
)

# Quality Reviewer Agent
quality_agent = Agent(
    role='Quality Reviewer',
    goal='Review, enhance and ensure completeness of generated documents',
    backstory='Senior consultant with expertise in quality assurance and document standards',
    tools=[rag_tool],
    llm=llm,
    verbose=True,
    callbacks=[log_handler]
)
```

### 2. Agent Task Pipeline
```python
# Research Task
research_task = Task(
    description=f"""
    Analyze the uploaded project documents to extract key information for {document_type}.

    Use the RAG tool to search for relevant content and the Graph tool to understand relationships.
    Focus on:
    - Technical architecture and components
    - Dependencies and integrations
    - Current state assessment
    - Migration considerations

    Provide a comprehensive analysis with specific details and evidence from the documents.
    """,
    agent=research_agent,
    tools=[rag_tool, graph_tool],
    expected_output="Detailed analysis report with extracted information and evidence"
)

# Content Structuring Task
content_task = Task(
    description=f"""
    Based on the research analysis, create a well-structured {document_type} document.

    Structure the content with:
    - Executive Summary
    - Technical Analysis
    - Recommendations
    - Implementation Plan
    - Risk Assessment
    - Appendices

    Ensure professional formatting and logical flow.
    """,
    agent=content_agent,
    tools=[rag_tool, graph_tool],
    expected_output=f"Professional {document_type} document in {output_format} format"
)

# Quality Review Task
quality_task = Task(
    description=f"""
    Review the generated {document_type} document for:
    - Completeness and accuracy
    - Professional presentation
    - Logical structure and flow
    - Technical correctness
    - Actionable recommendations

    Enhance the document with additional insights and ensure it meets enterprise standards.
    """,
    agent=quality_agent,
    tools=[rag_tool],
    expected_output=f"Final polished {document_type} document ready for delivery"
)
```

### 3. Real-time Agent Logging
```python
class AgentLogStreamHandler(BaseCallbackHandler):
    def on_agent_action(self, action, **kwargs):
        log_data = {
            "type": "agent_action",
            "timestamp": datetime.utcnow().isoformat(),
            "agent_name": self.current_agent.role,
            "tool": action.tool,
            "tool_input": str(action.tool_input),
            "action_description": f"{self.current_agent.role} is using {action.tool}"
        }

        # Send to WebSocket for real-time updates
        if self.websocket:
            asyncio.create_task(self.websocket.send_text(json.dumps(log_data)))

    def on_tool_end(self, output, **kwargs):
        log_data = {
            "type": "tool_result",
            "timestamp": datetime.utcnow().isoformat(),
            "agent_name": self.current_agent.role,
            "tool_output": str(output)[:500],  # Truncate long outputs
            "success": True
        }

        if self.websocket:
            asyncio.create_task(self.websocket.send_text(json.dumps(log_data)))
```

---

## üîß Service Dependencies & Health Checks

### Required Services for Document Generation
1. **PostgreSQL** (Port 5432) - Project data & LLM configurations
2. **Weaviate** (Port 8080) - Document embeddings for RAG
3. **Neo4j** (Port 7687) - Knowledge graph for relationships
4. **MinIO** (Port 9000) - Document storage
5. **Reporting Service** (Port 8001) - PDF/DOCX generation

### Required Services for Chat Functionality
1. **PostgreSQL** (Port 5432) - Project data & LLM configurations
2. **Weaviate** (Port 8080) - Document embeddings for semantic search
3. **Neo4j** (Port 7687) - Knowledge graph for contextual queries

### Health Check Endpoints
```python
@app.get("/health")
async def health_check():
    services = {
        "project_service": check_project_service(),
        "rag_service": check_weaviate_connection(),
        "weaviate": check_weaviate_direct(),
        "neo4j": check_neo4j_connection()
    }

    status = "healthy" if all(s == "connected" for s in services.values()) else "degraded"

    return {
        "status": status,
        "services": services,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }
```

---

## üìù Current Status & Recent Fixes

### Document Generation - FIXED ‚úÖ
1. **‚úÖ Reporting Service Connection**: Fixed MinIO connection (localhost:9000)
2. **‚úÖ Duplicate Processing**: Implemented deduplication logic with processing stats
3. **‚úÖ Local Storage**: Documents now saved both locally and in MinIO
4. **‚úÖ Agent Execution**: CrewAI agents properly configured with WebSocket logging
5. **‚úÖ Service Integration**: All services running on correct ports (8000, 8001, 8002)

### Chat Functionality - PARTIALLY FIXED ‚ö†Ô∏è
1. **‚úÖ RAG Service**: Properly initialized with project-specific LLM configuration
2. **‚úÖ Vector Search**: Weaviate connection working with project-specific collections
3. **‚úÖ Graph Queries**: Neo4j integration functional
4. **‚ö†Ô∏è LLM Integration**: Project-specific LLM configuration working but may need testing

### Infrastructure Status
1. **‚úÖ Docker Containers**: All required databases running (PostgreSQL, Neo4j, Weaviate, MinIO)
2. **‚úÖ Service URLs**: Updated to use localhost for local development
3. **‚úÖ Deduplication**: Implemented with processing statistics tracking
4. **‚úÖ Local & Cloud Storage**: Dual storage implementation complete
5. **‚úÖ Agent Pipeline**: CrewAI agents with real-time logging
6. **‚úÖ RAG Integration**: Vector and graph search operational
7. **‚úÖ LLM Configurations**: Database-persisted configurations working

---

## üöÄ Next Steps

1. **Analyze Current State**: Check all services and identify specific failures
2. **Start Required Containers**: Ensure PostgreSQL, Weaviate, Neo4j, MinIO are running
3. **Fix Service Connections**: Update configuration for local development
4. **Test Document Generation**: Verify end-to-end document creation pipeline
5. **Test Chat Functionality**: Verify RAG-based question answering
6. **Implement Improvements**: Add deduplication, local storage, better error handling
7. **Update Documentation**: Keep this document current with any changes

---

*This document should be updated whenever changes are made to the platform's data flow, database structure, or service architecture.*
