#!/usr/bin/env python3
"""
Import document template data from markdown file to PostgreSQL database
"""

import psycopg2
import uuid
from datetime import datetime
import re

# Database connection details
DB_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'projectdb',
    'user': 'projectuser',
    'password': 'projectpass'
}

# Template data from documenttemplatedata.md
TEMPLATE_DATA = [
    {
        'name': 'Cloud Readiness Scorecard',
        'description': 'A quantitative and qualitative scorecard rating the client\'s readiness for cloud migration across key dimensions.',
        'category': 'assessment',
        'format_details': 'Format: A structured scorecard. Must include: 1. Overall Readiness Score (out of 100). 2. Key Dimension Scores (Organizational, Technical, Security, Financial). 3. A Strengths, Weaknesses, Opportunities, Threats (SWOT) analysis section. 4. A summary of Top 3 Recommendations to improve readiness.',
        'output_format': 'pdf'
    },
    {
        'name': 'Current-State Technical Deep-Dive',
        'description': 'A comprehensive inventory and analysis of the client\'s current IT landscape as discovered from the provided documents.',
        'category': 'technical',
        'format_details': 'Format: A detailed technical document. Must include: 1. Application & Server Inventory (Table format). 2. Database Inventory (Table format). 3. Discovered Network & Dependency Map (Must include Mermaid diagrams for key application data flows). 4. Identified Technical Debt & End-of-Life (EOL) Systems.',
        'output_format': 'pdf'
    },
    {
        'name': 'Security Compliance Checklist',
        'description': 'A detailed security and compliance audit report, mapping discovered assets against specified regulatory frameworks.',
        'category': 'security',
        'format_details': 'Format: Checklist and tabular format. Must include: 1. Applicable Regulations (GDPR, HIPAA, etc.). 2. Data Classification Summary (PII/sensitive data locations). 3. Security Control Audit Table (Columns: Control, Asset, Status, Findings, Recommended Remediation). 4. Identity & Access Management (IAM) Review.',
        'output_format': 'pdf'
    },
    {
        'name': 'Total Cost of Ownership (TCO) & ROI Analysis',
        'description': 'A financial analysis comparing the costs of the current environment with the projected costs and benefits of the cloud.',
        'category': 'financial',
        'format_details': 'Format: A financial report for a CFO-level audience. Must include: 1. Current-State Annual Cost Breakdown. 2. Projected 3-Year Cloud Costs. 3. Migration Project Costs. 4. 3-Year TCO Comparison Chart. 5. Return on Investment (ROI) Calculation and Payback Period. 6. List of Qualitative Financial Benefits.',
        'output_format': 'pdf'
    },
    {
        'name': 'Target Cloud Architecture Proposal',
        'description': 'The detailed proposed target architecture on the chosen cloud provider. This is the primary technical blueprint.',
        'category': 'technical',
        'format_details': 'Format: A formal architecture document. Must include: 1. Recommended Cloud Provider & Justification. 2. High-Level Architecture Diagram (Embed the PNG generated by the Diagramming Agent). 3. Network Design (VPC/VNet layout). 4. Compute & Data Strategy. 5. Bill of Materials (List of proposed cloud services).',
        'output_format': 'pdf'
    },
    {
        'name': 'Detailed Migration Wave Plan',
        'description': 'A detailed project plan that groups applications into logical migration waves and outlines the sequence of events.',
        'category': 'migration',
        'format_details': 'Format: A project plan document. Must include: 1. Wave Definitions Table (Applications/servers per wave). 2. Dependency Justification for the wave structure. 3. High-Level Phased Timeline (e.g., Q1, Q2, Q3). 4. Key Milestones and Deliverables for each wave.',
        'output_format': 'pdf'
    },
    {
        'name': 'Executive Summary Briefing',
        'description': 'A concise, one-page briefing for executive stakeholders, focusing on business outcomes, ROI, and key strategic choices.',
        'category': 'reporting',
        'format_details': 'Format: A 1-2 page executive document. Must include: 1. Executive Summary (<200 words). 2. Key Business Drivers. 3. High-Level TCO & ROI Projection. 4. Top 3 Strategic Recommendations. 5. Project Phases & Timeline Diagram. Constraint: Avoid deep technical jargon.',
        'output_format': 'pdf'
    },
    {
        'name': 'Application-Specific Migration Playbook',
        'description': 'A focused, tactical plan for migrating a single critical application or a small group of related applications.',
        'category': 'migration',
        'format_details': 'Format: A step-by-step playbook. Must include: 1. Application Overview. 2. Chosen Migration Strategy (6Rs). 3. Pre-Migration Checklist. 4. Step-by-Step Execution Guide. 5. Validation & Testing Plan. 6. Rollback Procedure.',
        'output_format': 'pdf'
    },
    {
        'name': 'Cloud Provider Comparison Matrix',
        'description': 'An objective comparison of the top 3 cloud providers (AWS, Azure, GCP) based on the client\'s specific needs.',
        'category': 'migration',
        'format_details': 'Format: A comparison matrix in a table. Must include: 1. Comparison Criteria (e.g., Cost, Performance, Data Services, AI/ML Capabilities, Security). 2. A Rating (1-5) and Justification for each provider against each criterion. 3. A final "Recommended Provider" summary.',
        'output_format': 'pdf'
    },
    {
        'name': 'Gap Analysis Report',
        'description': 'A document that explicitly details the gaps between the current-state and the proposed target architecture.',
        'category': 'technical',
        'format_details': 'Format: A structured analysis report. Must include: 1. Technology Gaps (e.g., moving from Oracle to PostgreSQL). 2. Skill & People Gaps (e.g., lack of Kubernetes expertise). 3. Process Gaps (e.g., no CI/CD pipeline). 4. A Prioritized Recommendations table.',
        'output_format': 'pdf'
    },
    {
        'name': 'Identity & Access Management (IAM) Strategy',
        'description': 'A focused plan for managing user identities, access, and permissions in the new cloud environment.',
        'category': 'security',
        'format_details': 'Format: A security strategy document. Must include: 1. Proposed Identity Provider (e.g., Azure AD, Okta). 2. Role-Based Access Control (RBAC) Model. 3. Privileged Access Management (PAM) Strategy. 4. Multi-Factor Authentication (MFA) Policy.',
        'output_format': 'pdf'
    },
    {
        'name': 'FinOps & Cloud Governance Model',
        'description': 'A strategic document outlining how the client will manage their cloud costs and enforce governance post-migration.',
        'category': 'financial',
        'format_details': 'Format: A governance plan. Must include: 1. Cost Allocation & Tagging Strategy. 2. Budgeting and Forecasting Process. 3. Cost Optimization Pillars (e.g., Right-Sizing, Savings Plans). 4. Policy Enforcement Plan (e.g., using AWS Control Tower or Azure Policy).',
        'output_format': 'pdf'
    },
    {
        'name': 'Monthly Progress Report',
        'description': 'A template for recurring project status updates, designed to be generated automatically based on project state.',
        'category': 'reporting',
        'format_details': 'Format: A formal progress report. Must include: 1. Reporting Period. 2. Executive Summary of Progress. 3. Milestones Achieved This Period. 4. Upcoming Milestones. 5. Risks & Issues Log. 6. Budget vs. Actuals Summary.',
        'output_format': 'pdf'
    },
    {
        'name': 'Data Migration Strategy',
        'description': 'A deep-dive plan focusing solely on the complex task of migrating databases and data stores.',
        'category': 'technical',
        'format_details': 'Format: A technical data migration plan. Must include: 1. Data Source Inventory. 2. Data Target Mapping. 3. Chosen Migration Tooling (e.g., AWS DMS, Azure Data Factory). 4. Downtime Minimization Strategy. 5. Data Validation & Cutover Plan.',
        'output_format': 'pdf'
    }
]

def import_templates():
    """Import template data into PostgreSQL database"""
    try:
        # Connect to database
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        
        print("Connected to PostgreSQL database")
        
        # Clear existing global templates
        cursor.execute("DELETE FROM deliverable_templates WHERE template_type = 'global'")
        print(f"Cleared existing global templates")
        
        # Insert new templates
        inserted_count = 0
        for template in TEMPLATE_DATA:
            template_id = str(uuid.uuid4())
            now = datetime.utcnow()
            
            cursor.execute("""
                INSERT INTO deliverable_templates (
                    id, name, description, prompt, project_id, created_at, updated_at,
                    template_type, category, output_format, is_active, created_by,
                    template_content, usage_count, last_used
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                )
            """, (
                template_id,
                template['name'],
                template['description'],
                template['format_details'],  # Use format_details as prompt
                None,  # project_id is NULL for global templates
                now,
                now,
                'global',
                template['category'],
                template['output_format'],
                True,  # is_active
                None,  # created_by (NULL for system templates)
                template['format_details'],  # template_content
                0,  # usage_count
                None  # last_used
            ))
            
            inserted_count += 1
            print(f"Inserted: {template['name']}")
        
        # Commit changes
        conn.commit()
        print(f"\n✅ Successfully imported {inserted_count} global document templates!")
        
        # Verify import
        cursor.execute("SELECT COUNT(*) FROM deliverable_templates WHERE template_type = 'global'")
        count = cursor.fetchone()[0]
        print(f"✅ Verification: {count} global templates in database")
        
        cursor.close()
        conn.close()
        
    except Exception as e:
        print(f"❌ Error importing templates: {e}")
        if 'conn' in locals():
            conn.rollback()
            conn.close()

if __name__ == "__main__":
    import_templates()
